---
title: "End-to-End Deep Reinforcement Learning para el seguimiento de trayectorias sobre superficies tridimensionales"
author: "<b>Autor</b>: Luis Alberto Castillo Barrientos<br><b>Director de tesis</b>: Dr. Reyes Rios Cabrera"
date: "3 de febrero del 2023"
output: 
  ioslides_presentation:
    css: style.css
    logo: images/logo.png
    widescreen: true
    transition: 0.0
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Agenda

* Introducción
  - Motivación
  - Planteamiento del problema
  - Estado del arte
  - Hipótesis y objetivos
  - Contribución y aplicaciones
* Metodología propuesta
  - Descripción la metodología de entrenamiento
  - Enfoque para resolver la tarea
* Resultados
* Conclusiones y trabajo futuro

# Introducción

## Motivación {.flexbox .vcenter}

<div class="columns-2">
  <video width="100%" autoplay loop>
  <source src="videos/alphazero_2017_final.m4v" type="video/mp4"/>
  </video>
  <div align="center">
  Silver, David, et al. "Mastering chess and shogi by self-play with a general reinforcement learning algorithm." (2017).
  </div>
 
  <video width="100%" autoplay loop>
  <source src="videos/openai_five_dota_final.m4v" type="video/mp4"/>
  </video>
  <div align="center">
  Berner, Christopher, et al. "Dota 2 with large scale deep reinforcement learning." (2019).
  </div>
</div>

## Aprendizaje por refuerzo (RL) {.flexbox .vcenter}

<div align="center">
  <img src="images/rl-model.png" width="80%">
</div>

## Aprendizaje por refuerzo profundo (DRL) {.flexbox .vcenter}

<div align="center">
  <img src="images/rl-algos.png" width="90%">
</div>

## DRL aplicado a la robótica {.flexbox .vcenter}

<div class="columns-2">
  <video width="100%" autoplay loop>
  <source src="videos/shadow_hand_real_block_final.m4v" type="video/mp4"/>
  </video>
  <div align="center">
  Andrychowicz, OpenAI: Marcin, et al. "Learning dexterous in-hand manipulation." (2018).
  </div>
 
  <video width="100%" autoplay loop>
  <source src="videos/shadow_rubik_normal.m4v" type="video/mp4"/>
  </video>
  <div align="center">
  Akkaya, Ilge, et al. "Solving rubik's cube with a robot hand." (2019).
  </div>
</div>


## Planteamiento del problema {.flexbox .vcenter}

<div class="columns-2">
  Algunas de las complicaciones de aplicar directamente DRL a robots reales son:<br><br>
  <ul>
  <li>Ineficiencia de muestreo</li>
  <li>Riesgo de seguridad</li>
  <li>Es difícil reiniciar el entorno</li>
  </ul>
 
  <img src="images/robot-safety-zone.jpg" width="100%">
</div>

## Aprendiendo en simulación {.flexbox .vcenter}

<div class="columns-2">
  <video width="100%" autoplay loop>
  <source src="videos/openai_rubik.m4v" type="video/mp4"/>
  </video>
 
  <img src="images/open-rubik-overview.jpg" width="100%">
</div>

## Tiempo de entrenamiento en DRL en simulación {.flexbox .vcenter}

<div align="center">
  <img src="images/training-time.png" width="90%">
</div>

---

<div class="columns-2">
  "Learning dexterous in-hand manipulation" de OpenAI requirió de lo siguiente:
  
  - **Simulador**: MuJoCo
  - **Recursos (CPU)**: 6144 núcleos de CPU
  - **Recursos (GPU)**: 8 Nvidea V100 $\approx$ 896 teraFLOPS
  - **Entrenamiento**: 40 hrs
  - **Costo**: $ 7418.88 USD
 
  <video width="100%" autoplay loop>
  <source src="videos/shadow_hand_block.mp4" type="video/mp4"/>
  </video>
</div>


## Estado del arte {.flexbox .vcenter}

| Simulador | Física | Fotorrealista | DR  | ROS | GPU | Multi-GPU |
|-----------|:------:|:-------------:|:---:|:---:|:---:|:---------:|
| Gazebo	|   ✅   |  	❌   	| ❌  | ✅  | ✅  |	❌ 	|
| Pybullet  |   ✅   |  	❌   	| ❌  | ❌  | ✅  |	❌ 	|
| MuJoCo	|   ✅   |  	❌   	| ❌  | ❌  | ❌  |	❌ 	|
| Unity 	|   ❌   |  	✅   	| ❌  | ❌  | ✅  |	❌ 	|
| Isaac Gym |   ✅   |  	✅   	| ✅  | ✅  | ✅  |	✅ 	|

## Isaac Gym {.flexbox .vcenter}

<div class="columns-2">
  <video width="100%" autoplay loop>
  <source src="videos/isaac-gym-demo-01.m4v" type="video/mp4"/>
  </video>
  <video width="100%" autoplay loop>
  <source src="videos/isaac-gym-demo-02.m4v" type="video/mp4"/>
  </video>

  <img src="images/rl-compute-time-comparative.png" width="100%">


## Learning Dexterity (OpenAI) con Isaac Gym {.flexbox .vcenter}

<div class="columns-2">
+------------+------------------------------------------+------------+------------+
| Simulador  | Poder de cómputo                         | Tiempo     | Costo      |
|            |                                          |            |            |
|            | (CPU + GPUs)                             | (HRS)      | (USD)      |
+============+:========================================:+:==========:+:==========:+
| MuJoCo     | 6144 nucleos de CPU + 8 GPUs Nvidea V100 | 40         | 7,4188.88  |
+------------+------------------------------------------+------------+------------+
| Isaac Gym  | 320 nucleos de CPU + 1 GPU Nvidea A100   | 2          | 4.73       |
+------------+------------------------------------------+------------+------------+

<video width="100%" autoplay loop>
<source src="videos/isaac-gym-demo-03.m4v" type="video/mp4"/>
</video>
<div align="center">
Makoviychuk, Viktor, et al. "Isaac gym: High performance gpu-based physics simulation for robot learning." (2021).
</div>
</div>


## Hipótesis {.flexbox .vcenter}

"Es posible generar una política de control optimizada para la solución de una tarea de seguimiento de trayectorias sobre superficies tridimensionales usando un robot manipulador, mediante un ambiente de simulación física realista que provea las observaciones necesarias para el entrenamiento, empleando una combinación de técnicas de control tradicionales y algoritmos de DRL de forma eficiente, reduciendo el número de interacciones del agente y los recursos computacionales durante el entrenamiento"

## Objetivos {.flexbox .vcenter}

- Realizar una comparación entre los simuladores robóticos y un estudio de compatibilidad con los frameworks de ML.
- Proponer una arquitectura experimental que permita generar las observaciones necesarias para el entrenamiento de un agente que permita resolver la tarea asignada.
- Hacer una comparación del rendimiento de entrenamiento, utilizando la metodología propuesta, variando los parámetros de simulación.

## Contribución del proyecto {.flexbox .vcenter}

1. Desarrollo de una metodología de entrenamiento de modelos de DRL para resolver tareas de seguimiento de trayectorias en superficies tridimensionales de forma end-to-end.
2. Una política capaz de inferir la fuerza aplicada a los actuadores de un robot a partir de las imágenes de un sensor RGBD ubicado en el efector final.
4. Proposición de una arquitectura de entrenamiento que implementa un simulador robótico de vanguardia y un framework de DL de alto rendimiento de manera eficiente.
5. Reducción del tiempo y los recursos computacionales requeridos para el entrenamiento de modelos de DRL.

## Aplicaciones {.flexbox .vcenter}

<div class="columns-2">
  Aplicación directa de las políticas a la automatización de procesos industriales con robots:<br><br>
  <ul>
  <li>Soldadura</li>
  <li>Pintura</li>
  <li>Aplicación de aditivos</li>
  <li>Manufactura aditiva</li>
  </ul>
 
  <img src="images/welding-robot.jpg" width="80%">
  <img src="images/painting-robot.jpg" width="80%">
</div>


# Metodología

## Arquitectura del sistema de aprendizaje {.flexbox .vcenter}

<div align="center">
  <img src="images/learning-platform.png" width="90%">
</div>

## Observación {.flexbox .vcenter}

<div align="center">
  <img src="images/agent-env-obs.png" width="90%">
</div>

## Generador de datos sintéticos de entrenamiento {.flexbox .vcenter}

<div class="columns-2">
$$
\mathbf{C}(u) = \sum_{i=0}^{i<n} N_{i,p}(u) \mathbf{P}_{i}
$$

$$
N_{i, 0}(u)=\left\{\begin{array}{cc}
        1 & \text { si } u_{i} \leq u<u_{i+1} \\
        0 & \text { si no }
    \end{array}\right.
$$

$$
\begin{split}
N_{i, p}(u) = \frac{u-u_{i}}{u_{i+p}-u_{i}} N_{i,p-1}(u) + \cdots \\  \frac{u_{i+p+1}-u}{u_{i+p+1}-u_{i+1}} N_{i+1, p-1}(u)
\end{split}
$$

<img src="images/bspline.png" width="100%">
</div>

## Mapeo de la trayectoria 2D a 3D

<div class="columns-2">
$$
\begin{align*} 
\textbf{p}_{uv} &= -\begin{bmatrix}
        2 \pi s + \pi \\
        \pi t + \frac{\pi}{2}
    \end{bmatrix} \\\\
\textbf{p}_{xyz} &= r \begin{bmatrix} 
        \cos{(v)} \cos{(u)} \\
        \cos{(v)} \sin{(u)} \\
        \sin{(v)}
    \end{bmatrix} \\\\
\textbf{n} &= \begin{bmatrix} \sin{(u)} \cos{(v)} \\ \sin{(v)} \sin{(u)} \\ \cos{(u)} \end{bmatrix}
\end{align*}
$$

<img src="images/spherical-coord.png" width="100%">
</div>

## Domain Randomization {.flexbox .vcenter}

<div align="center">
  <img src="images/random-props.png" width="90%">
</div>

## Espacio de acción

<img src="images/training-system.png" width="90%">

$$
\tau = \textbf{J}_{ee}^{T}(\textbf{q}) \textbf{M}_{\textbf{x}_{ee}}(\textbf{q}) [k_{p}(\delta_x) + k_v(\delta_{\dot{x}})] + \textbf{g}(\textbf{q})
$$

## Señal de recompensa {.flexbox .vcenter}

$$
\begin{align*}
\text{Distancia} \rightarrow R_{d} &= -{\lVert \textbf{x}_{ee} - \textbf{x}_d \lVert}_2 \\\\
\text{Orientación} \rightarrow R_{o} &= -{\lVert \mathcal{H}(\textbf{q}_d, \textbf{q}^{*}_{ee}) \lVert}_2 \\\\
\text{Regularización} \rightarrow R_{c} &= -{\lVert \textbf{a} \lVert}^{2}_2 \\\\
\text{Recompensa final} \rightarrow R &= \alpha R_d + \beta R_o + \gamma R_c
\end{align*}
$$

## Arquitectura de red neuronal {.flexbox .vcenter}
 
<div align="center">
  <img src="images/nn-arch.png" width="90%">
</div>

## Algoritmo de entrenamiento (PPO-Clip)

PPO-Clip actualiza la política con:
$$
\theta_{k+1} = \underset{\theta}{\mathrm{arg \space max}} \space \underset{s,a \sim \pi_{\theta_{k}}}{\mathrm{E}} \space \left[ L(s, a, \theta_k, \theta) \right]
$$

Función objetivo:
$$
L(s, a, \theta_k, \theta) = \min{\left( \frac{\pi_{\theta}(a|s)}{\pi_{\theta_{k}}(a|s)} A^{\pi_{\theta_{k}}}(s,a), \space\space g(\epsilon, A^{\pi_{\theta_{k}}}(s,a)) \right)}
$$

Donde:
$$
 g(\epsilon, A) = \left\{\begin{array}{cc}
        (1 + \epsilon)A & \text { si } A \geq 0 \\
        (1 - \epsilon)A & \text { si } A < 0
    \end{array}\right.
$$

---

<div align="center">
  <img src="images/ppo-clip-algo.png" width="90%">
</div>

# Resultados

## Experimento: El efecto de $\lambda$ {.flexbox .vcenter}

<div align="center">
  <img src="images/exp-lambda.png" width="85%">
</div>

## Experimento: N. de entornos paralelos {.flexbox .vcenter}

<div align="center">
  <img src="images/results-envs.png" width="85%">
</div>

## Desempeño de la política {.flexbox .vcenter}

<div style="width: 100%; overflow: hidden;" align="center">
  <div style="width: 40%; float: left;">
  | Métrica                 |     Promedio      | Mediana |
  |-------------------------|:-----------------:|:-------:|
  | Error en posición (mts) | $0.01 \pm 0.0003$ | $0.011$ |
  | Error en orientación    | $0.08 \pm 0.001$  | $0.089$ |
  </div>
  <div style="margin-left: 42%;">
  <img src="images/results-error.png" width="100%">
  </div>
</div>

## Demo {.flexbox .vcenter}

<div style="width: 100%; overflow: hidden;" align="center">
  <div style="width: 72%; float: left;">
  <video width="100%" autoplay loop>
  <source src="videos/best-policy-demo-full.m4v" type="video/mp4">
  </video>
  </div>
  <div style="margin-left: 75%;">
  <img src="images/sample-01.gif" width="65%">
  <img src="images/sample-02.gif" width="65%">
  <img src="images/sample-03.gif" width="65%">
  </div>
</div>

# Conclusiones y trabajo futuro

## Conclusiones {.flexbox .vcenter}

- Este trabajo propone metodología para resolver el problema de seguimiento de trayectoria en superficies tridimensionales de forma end-to-end, mediante una combinación de algoritmos de DRL y técnicas de control tradicionales.
- La política generada tiene una tasa de éxito del **96%** y un error en la posición de **$\pm$ 0.01** metros.
- Se logró reducir en **30%** el tiempo de entrenamiento mediante la paralelización de 512 ambientes en una sola GPU Nvidia 3070.
- Este trabajo es pionero en resolver tareas de manipulación guiadas con visión en la plataforma de investigación de RL de Isaac Gym y podría habilitar el desarrollo de trabajos futuros en el área de DRL y visión por computadora.

## Trabajo futuro {.flexbox .vcenter}

- Ampliar el estudio a otros agentes manipuladores y el desarrollo de políticas de control generalizadas.
- Transferir la política aprendida en simulación a agentes reales (i.e., sim2real) mediante el uso de técnicas de Domain Randomization.
- Ampliar el alcance de la tarea a cualquier objeto 3D arbitrario.

## {data-background=images/learning-platform-alpha.png data-background-size=90% .flexbox .vcenter}

<h1>Gracias por su atención.</h1>